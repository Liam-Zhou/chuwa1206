### Q1
### Q2. Document the microservice architeture and components/tools/dependencies
- Architecture
  - Decentralized ccontrol: service are developed, deployed and manged independently
  - Business capability alignment: each microservice is aligned with a specific business function
  - Technology Diversity: allows using different technologies and languages for different services
  - Resilience: services are designed to be failure-resistant; if one service fails, the system continues to function
  - Scalability: services can be scaled independently to meet demand
- Component
  - Service discovery: manges and locates microservices in the network
  - API gateway: the entry point for clients, routing requests to appropriate services
  - Configuration management: externalizes and manages services configuration
  - Circuit breaker: handles failure and latency between services
  - Load balancer: distributes network or application traffic across multiple servers
- Tool
  - Containerization: docker, podman for creating and managing containers
  - Orchestration: Kubernetes, Docker Swarm for managing container's lifecycle
  - Continuous Integration/Continuous Deployment(CI/CD): Jenkins, GitLab Ci for automating the deployment
  - Monitoring and logging: Prometheus, Grafana for monitoring, Fluentd for logging
  - Tracing: Jaeger, Zipkin for distributed tracing and monitoring
- Dependencies
  - programming language
  - database: Relational, NoSQL
  - message system: kafka for asynchronous communication
  - security: JWT for authentication and authorization
  - API documentation: swagger
### Q3. What are Resilience patterns? What is circuit breaker?
- Resilience patterns
  - retry: attempts failed operations again
  - circuit breaker: blocks calls to a failing service to prevent further issues
  - timeout: sets a limit on how long operations can take
  - bulkhead: separates system parts to contain failures
  - fallback: uses alternatives when primary operation fail
  - rate limiter: controls the pace of incoming requests
  - cache: stores frequently accessed data temporarily
- Circuit Breaker: a mechanism that prevents calls to failing service, allowing it time to recover. It operates in three states: closed for normal operation, open to block calls and facilitate recovery, half-open to test for service restoration
### Q4. Read this article, then list the important questions, then write your answers
- features of microservices?
  - decoupling: within a system, services are largely decoupled
  - componentization: microservices are viewed as independent components that can easily be exchanged or upgraded
  - business capabilities: microservices are relatively simple and only focus on one service
  - team autonomy: each developer works independently of each other, allowing for a faster project timeline
  - continuous delivery: enables frequent software release through systematic automation of automation of software development, testing, and approval
  - responsibility: microservices are not focused on applications as projects, rather, they see applications as products they are responsible for
  - decentralized governance: choosing the right tool according to the job is the goal
  - agility: microservices facilitate agile development, it's possible to create new features quickly and discard them again at any time
### Q5. how to do load balance in microservice? Write a long Summary by yourself.
- load balancing in a microservice architecture involves distributing client requests or network load efficiently across multiple servers or instances of a service
- choose a load balancer type: hardware, software
- load balancing strategies: round-robin, least connections, ip hash, random
- implementing load balancing: DNS load balancing, reverse proxy, cloud-based load balancers
- service discovery integration: integrate with a service discovery tool to dynamically register and deregister service instance
- health checks: implement health checks to monitor the status of backend services
- SSL termination: consider where to terminate SSL connections
- Session management: for applications requiring session persistence, configure the load balancer to use sticky session, ensuring all requests from a particular client are directed to the same server
- test and monitor: regularly test the load balancing setup under various condition and monitor performance to adjust configurations as needed
### Q6. How to do service discovery?
- use a service registry: centralized registry, services register themselves upon startup and deregister on shutdown
- implement client-side discovery: services query the registry to find the locations of other services
- implement server-side discovery: a load balancer or API gateway queries the registry and routes requests to the appropriate service
- automate health check: the registry or a monitoring tool checks the health of services to update their availability status
- dynamic configuration: services update their registration dynamically as they scale up or down
### Q7. What are the major components of Kafka?
- producer: sends records to topics
- consumer: reads records from topics
- broker: kafka server that stores and serves data
- topic: categorizes records
- partition: splits topics for scalability
- zookeeper: manages kafka cluster
- consumer groups: logic grouping of consumers
- replicas: copies of partitions for fault tolerance
### Q8. What do you mean by a Partition in Kafka?
- it's a fundamental concept representing a subset of a topic's data. It's essentially a log sequence where records are stored and ordered by their offset
- scalability and parallelism: partitions allow a topic to scale by distributing data across multiple brokers in the cluster
- ordering guarantees: within a partition, messages are guaranteed to be in the oder they were sent
- fault tolerance: partitions can be replicated across multiple brokers, ensuring data availability and durability even if a broker fails
- consumer offsetting: consumers track their position in each partition, allowing them to resume consumption from where they left off 
### Q9. What do you mean by zookeeper in Kafka and what are its uses?
- zookeeper in kafka is an open-source configuration, synchronization service and naming registry for distributed systems.
- cluster management: tracks alive brokers and cluster membership
- topic management: stores configurations like partitions and replication factors
- leadership election: chooses leader brokers for partitions
- health checks: monitors broker health for consistent operation
- synchronization: ensures updates are consistently applied across the cluster
### Q10. Can we use Kafka without Zookeeper?
in kafka raft metadata mode, the brokers coordinate with each other using a consensus protocol called Raft, which enables them to elect a leader and replicate metadata across the cluster without relying on an external system like zookeeper 
### Q11. Explain the concept of Leader and Follower in Kafka.
- leader
  - role: each partition of a topic has one leader broker, the leader handles all read and write requests for the partition
  - election: the leader is automatically elected from the brokers that host a replica of the partition. in case of the leader's failure, a new leader is elected from the followers
  - responsibility: the leader broker ensures data consistency by managing the order of messages and coordinating updates with follower brokers
- follower
  - role: followers are brokers that replicate the dta of the leader partition but don't handle client requests directly
  - function: they passively replicate data from the leader and stand by to take over in case the current leader broker fails
  - health monitoring: followers regularly send heartbeats to leader to indicate they are alive and syncing
- this model can provide the mechanism for
  - load balancing
  - data replication
  - seamless failover
### Q12. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?
- topic replication: is vital for ensuring high availability and fault tolerance of the data within the kafka ecosystem
  - data durability
  - high availability
  - fault tolerance
  - load distribution
- ISR (In-Sync Replicas) is a concept closely related to topic replication in kafka and refers to the set of follower brokers that have fully replicated the log entries of the leader broker to the current point in time
### Q13. What do you understand about a consumer group in Kafka?
- it is a foundational concept that allows kafka to provide scalable and fault-tolerant processing of messages stored in topics
- concept
  - group of consumers
  - parallel processing
  - load balancing
- functionality
  - fault tolerance
  - scalability
- offset management
  - track progress
- use cases
  - real-time processing
  - load distribution
### Q14. How do you start a Kafka server?
- start zookeeper
  - navigate to kafka installation directory
  - use zookeeper start script with the default configuration file
- start kafka server
  - open a new terminal window or tab
  - start the kafka server using its start script and the default server configuration
### Q15. Tell me about some of the real-world usages of Apache Kafka.
- real-time analytics
  - it is used to ingest and process large streams of events or data in real time, enabling businesses to perform real-time analytics
- event sourcing
  - it serves as the backbone for event sourcing system, where every change to the application state is captured as an event and can be replayed to reconstruct the state or transfer the events to other systems
- log aggregation
  - it employed to collect logs from multiple services, applications, or servers, centralizing log data for monitoring, analysis, and compliance
- messaging
  - it is used as a messaging system to decouple data producers from consumers, ensuring reliable data exchange btwn different parts of an application
- stream processing
  - kafka streams API allows for building applications that can process streams of data in real-time, enabling complex processing, transformations and aggregations
### Q16. Describe partitioning key in Kafka.
partitioning key in kafka determines which partition a message is sent to within a topic, it ensures messages with the same key go to the same partition, preserving their order
### Q17. What is the purpose of partitions in Kafka?
- increase scalability: distribute data across multiple nodes, allowing for higher throughput
- enable parallel processing: multiple consumers can read from different partitions simultaneously, increasing performance
- ensure data redundancy: replicate data across brokers for fault tolerance
- maintain order: preserve the order of messages within each partition
### Q18. Differentiate between Rabbitmq and Kafka.
RabbitMQ is more suited for complex messaging scenarios with a focus on message delivery guarantees and routing features, while kafka is tailored for high-throughput, scalable data streaming applications where performance and data retention are key
### Q19. What are the guarantees that Kafka provides?
- at-most-once delivery
  - messages are delivered at most once and may not be delivered in case of failure
- at-least-once delivery
  - messages are guaranteed to be delivered at least once. if a failure occurs, messages may be delivered again, potentially causing duplicates
- exactly-once delivery
  - it ensures that each message is delivered exactly once, eliminating the risk of duplicates
### Q20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?
- it refers to a situation where the distribution of partitions and leaders across the kafka brokers is uneven
- use automatic rebalancing
- partition reassignment
- monitor and adjust
- balanced topic creation
- data clean-up
- use external tools
### Q21. In your recent project, are you a producer or consumer or both?
(for both condition) In my recent project, I had the opportunity to work as both a producer and a consumer. Initially, I focused on producing data, where I was responsible for collecting, formatting, and sending data to our Kafka cluster. As the project evolved, I also took on the role of a consumer, developing services to process and analyze this data for various business needs. Handling both sides gave me a comprehensive understanding of our data flow, from generation to consumption, and allowed me to ensure the overall efficiency and reliability of our streaming architecture.
### Q22. In your recent project, Could you tell me your topic name?
### Q23. In your recent project, How many brokers do you have? How many partitions for each topic? How many data for each topic.
In our recent project, we optimized our Kafka cluster for high availability and performance by deploying 5 brokers, allowing us to handle broker failures seamlessly. We tailored the number of partitions per topic, with up to 10 partitions for high-throughput topics, to maximize data processing parallelism and system performance. The data volume managed by our Kafka setup varied, with high-velocity topics processing about 500 GB daily and other critical topics around 100 GB daily. This configuration was dynamically monitored and adjusted to ensure efficient processing and scalability, demonstrating our proactive approach to system architecture and performance optimization.
### Q24. In your recent project, which team produce what kind of event to you and you producer what kind of events?
In our recent project, collaboration between backend teams was key to our data-driven architecture. The Order Management Team produced events related to order placements, updates, and status changes. These events were critical for real-time inventory management and customer notifications. Similarly, the Inventory Management Team generated stock level events, which were essential for automating restocking processes and preventing stockouts.
On my end, as part of the Data Processing Team, we consumed these events to aggregate order and inventory data, producing consolidated analytics events. These included summary reports on sales trends, inventory turnover rates, and order fulfillment efficiency. My role involved ensuring these processed events were accurately captured and made available in a format that was easily consumable by our Business Intelligence systems for further analysis and reporting.
This backend orchestration not only streamlined our internal processes but also enhanced our operational insights, allowing the business to react more swiftly to market and inventory changes.
### Q25. What is offset?
an offset in kafka is a sequential identifier that marks the position of each record within a partition, allowing consumers to track which messages have been read and manage their progress accordingly