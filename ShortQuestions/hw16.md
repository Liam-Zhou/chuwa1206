2. Document the microservice architeture and components/tools/dependencies

    - API Gateway: Zuul, Spring Cloud API
    - Service Discovery: Eureka, Consul
    - Config center: Spring Cloud Config
    - Messaging and Events: Kafka
    - Monitoring and Logging: Kibana, Elasticsearch
    - Load balancing: Nginx, Ribbon
    - Resilience: Hystrix, Resilience4j


3. What are Resilience patterns? What is circuit breaker?

    - Resilience patterns are design strategies used in software architecture to handle failures and ensure that a system remains functional and available, even when parts of it fail.
    - The Circuit Breaker pattern is a resilience and stability design pattern used to prevent a system from performing operations that are likely to fail. The pattern is named after an electrical circuit breaker that cuts off the electrical flow when a fault is detected, to prevent damage and fire.
      In the context of software, a circuit breaker monitors the number of recent failures for a particular operation or service call. If the failures exceed a certain threshold, the circuit breaker "trips" and the operation is temporarily disabled. During this time, all attempts to invoke the operation will fail immediately without the operation being executed
    - The circuit breaker has three states: 
        1. Closed: requests are allowed to pass through.
      2. Open: all requests fail immediately without executing the operation.
      3. Half-Open: a limited number of test requests are allowed to pass through.If these requests succeed, it's assumed that the issue is resolved, and the breaker returns to the closed state. If these requests fail, the breaker goes back to the open state.


4. Read this article, then list the important questions, then write your answers
   a. https://www.interviewbit.com/microservices-interview-questions/#main-features-of-microservices

    1. What are the benefits and drawbacks of Microservices?
   
      - benefits:
         1. Scalability: Each service can be scaled
            independently, thus managing resources
            more efficiently.
        2. Fault Isolation: If one service fails, it
           doesn't directly affect the other services.
        3. Technological Freedom: Each service can
           be developed using a different technology
           stack, according to the requirement.
        4. Easier Deployment and Updates: Services
           can be deployed and updated
           independently without affecting the entire
           system.
      - drawbacks:
        1. Complexity: Microservices can be more
           complex to manage due to the distributed
           nature of the architecture
        2. Data Management: Handling data
           consistency can be complex as each service
           can have its own database
        3. Network Latency: As services interact
           through network calls, latency can be
           higher compared to monolithic
           architectures.


5. how to do load balance in microservice? Write a long Summary by yourself.
   a. https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/
   b. https://www.fullstack.cafe/blog/load-balancing-interview-questions

   1. benefits of load balancer:
        1. Load balancers minimize server response time and maximize throughput.
        2. Load balancer ensures high availability and reliability by sending requests only to online servers
        3. Load balancers do continuous health checks to monitor the serverâ€™s capability of handling the request.
   2. Where Are Load Balancers Typically Placed?
        1. between the client application and the front-end server
      2. between the front-end server and the application/job servers
      3. between the application servers and the cache servers
      4. between the cache servers the database servers
   3. Types of Load Balancers
      1. Software Load Balancers in Clients: provided with a list of web servers/application servers to interact with
      2. Software Load Balancers in Services:  pieces of software that receive a set of requests and redirect these requests according to a set of rules.
      3. Hardware Load Balancers: These load balancers are also known as Layer 4-7 Routers and these are capable of handling all kinds of HTTP, HTTPS, TCP, and UDP traffic
   4. Load Balancing Algorithms
       1.  Round Robin
      2. Weighted Round Robin
      3. Least Connection Method
      4. Least Response Time Method
      5. Source IP Hash

6. How to do service discovery?

    - Registration: When a microservice instance starts, it registers itself with the service registry, providing essential information such as its network address (IP and port),
    - Discovery: Microservices query the service registry to find the network locations of other services they depend on.
    - Popular Service Registry Implementations: Eureka, Consul


7. What are the major components of Kafka?

   - Producer: Producers are responsible for publishing data or records to Kafka topics. Producers can choose the topic to which they want to send messages.
   - Broker: Kafka brokers are the servers that store and manage the published data. Brokers are responsible for managing topics, partitions, and replication.
   - Topic: Topics are logical channels or categories to which data is published. Producers send messages to specific topics. Consumers subscribe to topics to read messages.
   - Partition: Topics are divided into partitions to allow for parallelism and scalability. Each partition is an ordered, immutable sequence of records.
   - Consumer: Consumers subscribe to one or more topics to read and process messages. 
   - Consumer Group: Consumer groups are a way to distribute the processing of messages across multiple consumers.


8. What do you mean by a Partition in Kafka?

   - Kafka topics are divided into one or more partitions, allowing the distribution of data across multiple nodes within a Kafka cluster.
    1. Data Segmentation: A Kafka topic can be divided into one or more partitions. Each partition is essentially an ordered, immutable sequence of messages.
   2. Parallelism and Scalability: Partitions provide a level of parallelism and scalability. Multiple consumers can read from different partitions concurrently, allowing Kafka to process a large number of messages simultaneously.
   3. Ordering Guarantee: Messages within a partition are strictly ordered. This means that the order of messages within a partition is preserved
   4. Fault Tolerance: Kafka provides fault tolerance by replicating each partition across multiple Kafka brokers


9. What do you mean by zookeeper in Kafka and what are its uses?

   - It plays a crucial role as a coordination and metadata management service. Kafka uses ZooKeeper to maintain the cluster's overall health, manage broker metadata, and facilitate leader election for partitions.
      1. Cluster Coordination: ZooKeeper helps ensure that brokers are aware of each other's existence and health status(coordination and synchronization).
      2. Broker Metadata: Kafka brokers register themselves and store metadata such as their broker ID, host, and port information in ZooKeeper. This metadata allows clients and other brokers to discover and connect to the appropriate Kafka brokers when they need to produce or consume data.
     3. Leader Election: ZooKeeper is used for leader election within Kafka partitions. In Kafka, each partition has one leader and multiple followers (replicas). ZooKeeper helps in the election process by ensuring that a leader is chosen among the available replicas and that leadership can be transferred when needed


10. Can we use Kafka without Zookeeper?

   - Kafka traditionally relies on Apache ZooKeeper for various aspects of cluster coordination and metadata management.
   - But starting with Kafka version 2.8.0, it became possible to run Kafka without Zookeeper in a mode called KRaft (Kafka Raft Metadata mode). KRaft mode is a Kafka internal replacement for Zookeeper, which simplifies the architecture by removing the external dependency on Zookeeper. This mode uses a built-in consensus mechanism based on the Raft protocol to manage cluster metadata and other functions that Zookeeper used to handle.


11. Explain the concept of Leader and Follower in Kafka.

   - For each partition, Kafka elects one broker to be the leader and one or more brokers to be the followers.
   - Leader Partition: The leader partition is responsible for handling all read and write requests for that specific partition. When messages are produced to a partition, they are written to the leader partition first. The leader then ensures the data is replicated to all the follower partitions.
   - Follower Partitions: Follower partitions replicate the data from the leader partition. Followers read messages from the leader and write them to their own log. Follower partitions do not serve client requests directly; they only answer to the leader for the purpose of data replication. However, in the event of a leader failure, one of the follower partitions can be elected as the new leader, ensuring the availability of the partition.


12. Why is Topic Replication important in Kafka? What do you mean by ISR in
    Kafka?

   - High Availability
   - Fault Tolerance
   - Durability
   - Improved Read Scalability: While all writes go through the leader partition, consumers can be configured to read from follower partitions, distributing the read load and improving read scalability in high-throughput environments.

   - In-Sync Replicas (ISR) refers to the set of follower partitions that are fully up-to-date with the leader partition. To be considered in-sync, a follower must meet the following conditions:
      1. It must have fetched all messages from the leader up to a point that is close to real-time (configurable by a threshold).
     2. It must acknowledge writes within a configurable time window.

13. What do you understand about a consumer group in Kafka?

   - Each Consumer Group is made up of one or more consumers that work together to consume and process messages. It allows for scalable and fault-tolerant processing of messages from topics
   - Key Characteristics:
     1. Scalability: consume messages in parallel
     2. Load Balancing: Kafka automatically distributes the partitions of a topic across the consumers in a group.
     3. Fault Tolerance:  Kafka will reassign the partitions that were being consumed by the failed consumer to the remaining consumers in the group.
     
14. How do you start a Kafka server?

    1. Start ZooKeeper ```bin/zookeeper-server-start.sh config/zookeeper.properties```
    2. Start Kafka Broker ```bin/kafka-server-start.sh config/server.properties```


15. Tell me about some of the real-world usages of Apache Kafka.

    1. Event Streaming and Processing: For example, financial institutions use Kafka for real-time fraud detection by analyzing transaction events as they occur.
    2. Data Integration: Kafka is widely used as a central hub for data integration, connecting disparate systems.  It allows organizations to consolidate data from various sources, including databases, cloud services, and SaaS platforms, and stream this data efficiently to data lakes, data warehouses, or other systems for analysis, reporting, and machine learning.
    3. Microservices Communication: Kafka serves as a highly scalable and reliable messaging system that facilitates asynchronous communication and decoupling between microservices. By using Kafka, developers can ensure that microservices can exchange data and events reliably, even in the face of service failures or network issues, which enhances the overall resilience and scalability of the system.


16. Describe partitioning key in Kafka.

    -  a partition key determines to which partition a message is sent within a topic, ensuring messages with the same key are routed to the same partition. This is crucial for maintaining the order of related messages and enabling efficient data distribution across the cluster for scalability and fault tolerance.


17. What is the purpose of partitions in Kafka?

    - Partitions in Kafka enable parallelism, scalability, and fault tolerance by distributing data across multiple brokers in a cluster. They allow topics to be split into smaller segments, supporting concurrent reads and writes, and ensuring high throughput. 



18. Differentiate between Rabbitmq and Kafka.

    1. Message Delivery and Consumption
       - RabbitMQ: Supports multiple messaging models, including point-to-point, publish/subscribe, and request/reply patterns.
       - Kafka: Uses a publish/subscribe model
    2. Use Cases
       - RabbitMQ: Ideal for scenarios requiring complex routing, RPC (Remote Procedure Call), and where the emphasis is on message delivery guarantees and intricate message patterns.
       - Kafka: Best suited for large-scale message processing applications, real-time streaming analytics
    3. Scalability
        - RabbitMQ: Can scale horizontally, but scaling and performance can be impacted by message size, persistence requirements, and the complexity of routing.
        - Kafka: Built for high throughput and scalability from the ground up, with partitioning and replication as core components of its architecture. Kafka can handle very high volumes of data by distributing it across a cluster of servers.
    4. Durability
        - RabbitMQ: storing large amounts of data or messages for long periods is not its primary design goal.
        - Kafka: Designed to hold large amounts of data for a long time.
    


19. What are the guarantees that Kafka provides?

    1. Message Order, ensuring that messages are stored and read from a topic-partition in the order they were received;
    2. Fault Tolerance, achieved through data replication across multiple brokers, ensuring no data loss if a broker fails (with replication factor N, tolerate up to N-1 brokers being down);
    3. As long as the number of partitions remains constant for a topic, the same key will always go to the same partition.
    4. At-Least-Once Delivery, where messages are guaranteed to be delivered at least once, with configurations available to achieve exactly-once delivery semantics in specific scenarios.


20. What do you mean by an unbalanced cluster in Kafka? How can you balance it?

    - An unbalanced Kafka cluster occurs when partitions and leaders are unevenly distributed across brokers, leading to skewed workloads where some brokers are overburdened while others are underutilized. This can affect performance and reliability. Balancing can be achieved by reassigning partitions across brokers more evenly, using tools like Kafka's built-in partition reassignment tool or automated rebalancing features provided by Kafka management platforms, like `kafka-reassign-partitions.sh`.


21. In your recent project, are you a producer or consumer or both?

    - In my recent project, I played the role of both a producer and a consumer. As a producer, I was responsible for sending processed data to various topics for downstream analytics and reporting. As a consumer, I ingested raw data from upstream services, processed it, and then forwarded it for further processing or storage.

22. In your recent project, Could you tell me your topic name?

    - In my recent project, we primarily focused on a Kafka topic named "OrderTransactions". This topic was designed to stream real-time order and transaction data from our e-commerce platform, capturing details like order creation, updates, and finalizations, which were crucial for our order management and real-time inventory tracking systems.


23. In your recent project, How many brokers do you have? How many partitions for
    each topic? How many data for each topic.

    - In my recent project, our Kafka cluster was configured with 11 brokers to ensure robustness and scalability. For key topics like "UserActivityLogs", we set up 10 partitions each to facilitate efficient data processing and parallelism. On average, the "UserActivityLogs" topic handled around 2 million messages per day, amounting to roughly 100GB of data, accommodating our platform's high user activity volume.
    

24. In your recent project, which team produce what kind of event to you and you
    producer what kind of events?

    - The Frontend Team produced user interaction events, such as clicks and page views, that were published to the "UserActivityLogs" topic. My team, in turn, processed these events to derive insights like user engagement metrics and session durations, which we then published to a "ProcessedUserMetrics" topic for the Analytics Team to consume and generate reports.


25. What is offset?

    - In Kafka, an offset is a unique identifier for each message within a partition, representing the position of the message in that partition. Offsets are sequential numbers that Kafka uses to maintain the order of messages. Consumers track their progress in a partition by storing the offset of the last message they have processed, allowing them to resume consumption from the correct position, even after restarts or failures, ensuring no message is missed or processed twice.
      Two consumers in the same consumer group share the same partition assignment but maintain their own offset tracking independently. This means that within a consumer group, each consumer keeps track of its own offsets for the partitions it is assigned to. This allows each consumer to know which messages it has processed and which it needs to process next. 
      However, if two consumers belong to different consumer groups, they maintain completely separate offset tracking for the partitions they consume from. 
